---
title: 分布式系统
date: 2020-10-22 18:25:39
tags:
---
这篇学习笔记是针对 MIT 6.824 Distributed System. B站链接 https://www.bilibili.com/video/av91748150/

# 分布式系统
* 核心是通过网络是一群计算机相互通信来完成一些连贯的任务
* 使用分布式系统的原因
    * 实现并行
    * 容错
    * 物理上的原因（两台计算机处于不同的地理位置）
    * 考虑到安全性
* 分布式系统的挑战
    * 部分故障
    * 提升性能

## 基础架构
* 存储 (最为关注)
* 通信
* 计算

目标：能够从分布式存储和计算(computation)基础结构中发现一些可抽象的东西并设计为接口以简化使用

# Implementation
## RPC
## threads
## concurrency

# 性能 Performance
## 可扩展性 scalability
使用两倍的计算机或资源就能使我获得两倍的性能或吞吐量

# 容错 Fault Tolerance
## 可用性 Availability
## 可恢复性 Recoverability
## 非易失性存储 Non-volatile memory
## 复制 Replication
目前流行的三种变更复制的算法: ```单领导者```, ```多领导者```, ```无领导者```
### 领导者与追随者
存储数据库副本的每一个节点称为**副本(replica)**.

* 基于领导者的复制 (leader-based replication)
  * 其中一个副本被指定为 leader. 当客户端要向数据库写入时, 他必须将请求发送给 leader, leader 会将新数据写入器本地存储.
  * 其他副本被称为 followers. 每当 leader 将新数据写入本地存储时, 他也会将数据变更发送给所有的 followers, 这称为**复制日志(replication log)**. 每个 follower 从 leader 拉取日志, 并相应更新其本地数据库副本, 按照 leader 处理的相同顺序应用所有写入.
  * leader 是有读写操作的, 而 follower 只有读操作.

#### 同步复制与异步复制
复制的一个重要细节就是: 同步还是异步的.

> **半同步**: 由于从库可能存在(崩溃, 网络故障)等一系列原因, 将所有从库都设置成同步是不切实际的. 实际上, 如果在数据库上启用同步复制, 这意味着通常只有一个 follower 是同步的, 而其他的都是异步的. 如果发现同步的 follower 变得不可用或是缓慢, 使一个异步的 follower 同步. 这样就可以保证至少有在两个节点上拥有最新的数据副本.

#### 设置新从库
1. 在某个时刻获取主库的一致性快照, 而不必锁定整个数据库.
2. 将快照复制到新的从库节点
3. 从库连接到主库, 并拉取快照之后发生的所有数据变更. 这要求快照与主库复制日志中的位置精确关联.
4. 当从库处理完快照之后积压的数据变更. 现在可以继续处理主库上的数据变化了.

#### 处理节点宕机

* 从库失效: 追赶恢复
* 主库失效: 故障切换
    * 其中一个从库需要被提升为新的主库, 需要重新配置客户端, 以将他们的写操作发送给新的主库, 其他从库需要开始拉取来自新主库的数据变更.

#### 复制日志的实现
* 基于语句的复制
  * 任何调用非确定性函数的语句都会在每个副本上生成不同的值.例如 ```NOW()```, ```RAND()```
  * 如果语句使用了自增列, 则必须在每个副本上按照完全相同的顺序执行.
  * 有副作用的语句可能会在每个副本上产生不同的副作用.
* 传输预写式日志
  * 日志都是包含所有数据库写入的仅追加字节序列.
* 逻辑日志复制
  * 以行的粒度描述对数据库表的写入
    * 对于插入的行，日志包含所有列的新值。
    * 对于删除的行，日志包含足够的信息来唯一标识已删除的行。通常是主键，但是如果表上没有主键，则需要记录所有列的旧值。
    * 对于更新的行，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少所有已更改的列的新值）。

### 复制延迟的问题
当应用程序从异步从库中读取数据的时候, 如果从库落后, 它可能会看到过时的消息. 但这种不一致只是一个暂时的状态 —— 如果停止写入数据库并等待一段时间, 从库最终会赶上主库并保持一致. 这种效应被称为 **最终一致性 (eventually consistency)**

* 读写一致性
  * 如果用户重新加载页面, 他们总会看到他们自己提交的任何更新。
  * 读用户**可能已经修改过**的内容时, 强制都从主库读. (例如: 社交网络上的用户个人资料信息通常只能由用户本人编辑，而不能由其他人编辑. 因此一个简单的规则就是: 从主库读取用户自己的档案，在从库读取其他用户的档案)
  * 如果应用中的大部分内容都被用户编辑了
    * 可以跟踪上次更新时间, 在上次更新后的一分钟内, 从主库读. 
    * 也可以监控从库的复制延迟, 防止向任意滞后超过一分钟的从库发起查询.

* 单调读
  * 用户首先从新副本读取, 然后从旧副本读取. 可能会发现前后读取不一致的情况
  * 确保每个用户总是从同一个副本进行读取 (不同的用户可以从不同的副本读取)

* 一致前缀读
  * 如果一系列写入按某个顺序发生, 那么任何人读取这些写入时, 也会看见它们以同样的顺序出现.
  * 确保任何因果相关的写入都写入相同的分区.

### 多主复制
基于领导者的复制有一个主要的缺点：只有一个主库，而所有的写入都必须通过它。如果出于任何原因（例如和主库之间的网络连接中断）无法连接到主库, 就无法向数据库写入。

多领导者配置可以在每个数据中心都有主库, 每个数据中心内部使用常规的主从复制; 在数据中心之间, 每个数据中心的主库都会将其更改复制到其他数据中心的主库中.

#### 处理写入冲突
多领导者复制最大问题是可能发生写冲突, 这意味着需要解决冲突.

* 避免冲突
  * 如果应用程序可以确保特定记录的所有写入都通过同一个 leader , 那么这个冲突就不会发生.
* 收敛至一直的状态
  * 给每个写入一个唯一的 ID, 挑选最高 ID 的写入作为胜利者, 并丢弃其他写入. (LWW, last write wins)
  * 以某种方式将这些值合并在一起.
  * 在保留所有信息的显式数据结构中记录冲突, 并编写解决冲突的应用程序代码

#### 多主复制拓扑
有两个以上的 leader, 各种不同的拓扑时可能的.

{% asset_img 多领导拓扑.png 多领导拓扑 %}

* 循环和星型拓扑的问题是, 如果只有一个结点发生故障, 则可能会中断其他节点之间的复制消息流, 导致无法通信, 直到节点修复.
* 全能拓扑也可能有问题. 一些网络连接可能比其他网络连接更快, 一些消息可能先于它所依赖的消息.
  * 可以使用版本向量(version vector)来解决.

### 无主复制
客户端直接将写入发送到几个副本中. 在无领导配置中, 故障切换不存在, 为了解决这个问题, 当一个客户端从数据库中读取数据的时候, 它不仅仅发送他的请求到一个副本; 读请求也被并行的发送到多个节点. 客户可能会从不同的节点获得不同的响应。即来自一个节点的最新值和来自另一个节点的陈旧值。版本号用于确定哪个值更新.

* 法定人数
  * 如果有 n 个副本, 每个写入必须由 w 个节点确认才能被认为是成功的, 并且我们必须至少为每一个读取查询 r 个节点. (w + r > n) 即可以保证在读取的节点中一定能够读取到最新的值.


如果两个操作都互相感觉不到对方的存在, 就称这两个操作**并发**.

服务器可以通过查看版本号来确定两个操作是否并发的
1. 服务器为每个键保留一个版本号，每次写入键时都增加版本号，并将新版本号与写入的值一起存储.
2. 当客户端读取键时，服务器将返回所有未覆盖的值以及最新的版本号。客户端在写入前必须读取.
3. 客户端写入键时，必须包含之前读取的版本号，并且必须将之前读取的所有值合并在一起.
4. 当服务器接收到具有特定版本号的写入时，它可以覆盖该版本号或更低版本的所有值, 但是它必须保持所有值更高版本号.

### 参考文献
* [设计数据密集型应用-第五章](https://vonng.gitbooks.io/ddia-cn/content/ch5.html)

## 分区 Partition
分区是一种有意将大型数据库分解成小型数据库的方式.

> tips: 上文中的**分区(partition)**,在MongoDB,Elasticsearch和Solr Cloud中被称为**分片(shard)**,在HBase中称之为**区域(Region)**，Bigtable中则是**表块(tablet)**，Cassandra和Riak中是**虚节点(vnode)**, Couchbase中叫做**虚桶(vBucket)**.但是分区(partition) 是约定俗成的叫法。

### 分区与复制
不均衡导致的高负载的分区被称为热点(hot spot). 解决这个问题可以采用一些的办法

* 根据键的范围分区
  * 为每个分区指定一块连续的键范围 (分区边界可以由管理员手动选择, 也可以由数据库自动选择)
  * 缺点
    * 某些特定的访问模式会导致热点. (e.g., 主键是时间戳的话)
* 根据键的散列分区
  * 一个好的 hash function 可以将偏斜的数据均匀分布.
  * Java 中的 Object.hashCode() 和 Ruby 的 Object#hash, 同一个键可能在不同的进程中有不同的哈希值.
  * 一致性哈希 **(Consistent Hashing)** 
    * [一致性哈希算法 consistent hashing](http://www.zsythink.net/archives/1182)
* 负载倾斜与消除热点
  * 如果一个主键被认为是非常火爆的, 可以在这个主键的开始或结尾添加一个随机数.
  * 需要一些方法来跟踪哪些键需要被分割(否则对于吞吐量低的绝大多数主键来是不必要的开销)

### 分片与次级索引
#### 按文档的次级索引

{% asset_img 按文档的二级索引.png 按文档的二级索引 %}

每个分区完全独立: 每个分区维护自己的二级索引, 仅覆盖该分区中的文档. 所以也被称为**本地索引(local index)**.

缺点: 可能会使二级索引上的读取查询相当昂贵.
#### 基于关键词的次级索引

{% asset_img 按关键词的二级索引.png 按关键词的二级索引 %}

构建一个覆盖所有分区的全局索引, 而不是给每个自己分区创建自己的本地索引.

缺点: 写入速度较慢且较为复杂, 因为写入单个文档现在可能会影响索引的多个分区
### 分区再平衡
再平衡一般需要满足:
* 再平衡之后, 负载(数据存储, 读取和写入请求)应该在集群中的节点之间公平地共享.
* 再平衡发生时, 数据库应该继续接受读取和写入
* 节点之间只移动必须的数据, 以便快速再平衡, 并减少网络和磁盘 I/O 负载.

#### 固定数量的分区
创建比节点更多的分区, 并为每个节点分配多个分区. 如果一个节点被添加到集群中, 新节点可以从当前每个节点中拿走一些分区, 直到分区再次公平分配. 如果从集群中删除一个节点, 就会把节点中的分区再分配回去.

{% asset_img 固定数量的分区.png 固定数量的分区 %}

缺点: 数据集的总大小难以估计, 难以选择正确的分区数.
#### 动态分区
按键的范围进行分区的数据库会动态创建分区. 当分区增长到超过配置的大小时, 会被分成两个分区, 每个分区约占一般的数据. 相反的, 如果大量数据被删除并且分区缩小到某个阈值以下, 则可以将其与相邻分区合并.

#### 按节点比例分区
以上两种分区方式, 分区的数量都与节点的数量无关.

每个分区的大小与数据集大小成比例地增长, 而节点数量保持不变, 但是当增加节点数时, 分区将再次变小. 由于较大的数据量通常需要较大数量的节点进行存储, 因此这种方法也使每个分区的大小较为稳定.


# 分布式系统的麻烦
* 无法访问的网络
* 时钟和时序问题

## 故障与部分失效
在分布式系统中, 尽管系统的其他部分工作正常, 单系统的某些部分可能会以某种不可预知的方式被破坏. **(partial failure 部分失效)**

### 不可靠的网络


#### 检测故障
* 负载平衡器需要停止向已死亡的节点转发请求
* 在单主复制功能的分布式数据库中, 如果主库失效, 则需要将从库之一升级为新主库.



# 一致性与共识 Consistency and Consensus

**共识(consensus)** : 就是让所有的节点对某件事达成共识.

## 一致性保证

### 线性一致性 = 强一致性
让系统看起来好像只有一个数据副本, 而且所有的操作都是原子性的.

#### 区别 线性一致性(Linearizability) vs. 可序列化(Serializability)
* 可序列化
  * Serializability 是事务的隔离属性.
  * 它确保事务的行为, 与它们按照**某种**顺序依次执行的结果相同. 这种执行顺序可以与事务实际执行的顺序不同.
  * 保证即使事务可以并行执行, 最终的结果也是一样的, 就好像它们没有任何并发性, 连续挨个执行一样.
* 线性一致性
  * 不会将操作组合成事务. 是在单个对象上面的单个操作保证
